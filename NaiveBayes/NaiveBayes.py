# 朴素贝叶斯对条件概率做出了条件独立的假设
# input: n dim vector
# output: classes(c1, c2, c3, .. ,ck)
# train data is generated by the P(X, Y) 独立同分布

# need attention: 1. prior probability； P(Y=ck), 2. 条件概率：P(X=x|Y=ck)
# 这种方法并不适用于维数特别大，取值特别多的情况，且必须提前得知取值范围。对于出现概率为0的情况，需要采用laplace平滑处理，
# 该程序并未对未出现的特征数值进行处理，因此概率不高
import math


class NaiveB:
    """
    A realisation of naive bayes
    """
    def __init__(self, classes):
        """
        :param classes: the values of classes, example: [1,2,3,4,5]
        :param feature_values: a dictionary, key is the index of the dimension, value：the possible value of the dimension
            shape: (number_feature, possible_value_feature)
        """
        self.classes = classes
        self.pro_priors = [0 for i in range(len(classes))]
        self.pro_cons = []

    def train(self, train_data, train_label):
        """
        :param train_data: the datasets of train, an list, (number_data, features)
        :param train_label: the label of according train dataset, an list, (number_data)
        Update the pro_prior and pro_con parameter
        """
        number_pro = [Probability(name) for name in self.classes]
        number_label = [0 for i in range(len(self.classes))]

        for i in range(len(train_label)):
            label_index = self.classes.index(train_label[i])    # determine the class
            number_label[label_index] += 1                   # class += 1
            for j in range(len(train_data[i])):            # the dimension of each class
                if j not in number_pro[label_index].proba:
                    number_pro[label_index].proba[j] = [train_data[i][j]]
                else:
                    number_pro[label_index].proba[j].append(train_data[i][j])

        # print('number_pro: ', number_pro[1].proba)
        # get the prior probability
        for i in range(len(self.classes)):
            self.pro_priors[i] = number_label[i]/len(train_label)
        # print('pro_priors:', self.pro_priors)

        for pro in number_pro:
            pro.get_proba()

        # result = [pro1.prob for pro1 in number_pro]
        # print(result)
        self.pro_cons = number_pro

    def predict(self, test_data, test_label):
        # test_data: the dataset of the test data
        # test_label: the label of the test data
        predict_label = []
        for j in range(len(test_data)):
            probability_classes = []
            for i in range(len(self.classes)):
                p = self.pro_priors[i]
                for m in range(len(self.pro_cons[i].prob)):
                    # print('p1:', p)
                    p *= self.pro_cons[i].prob[m].get(test_data[j][m], test_data[j][-1]) # 第i个类
                    # print('p:', p)
                probability_classes.append(p)

            n = probability_classes.index(max(probability_classes))
            predict_label.append(self.classes[n])

        correct = 0
        for i in range(len(test_label)):
            if predict_label[i] == test_label[i]:
                correct += 1
        return correct / len(test_label)


class Probability:
    def __init__(self, class_name):
        self.class_name = class_name
        self.proba = {}         # {dimension i: [the values in the dimension]}
        self.prob = []          # [{value: probability}, {}, ..., {}]

    def get_proba(self):
        # prob: (num_features)
        total_label = len(self.proba[0])    # the number of data in the class
        prob = [{} for i in range(len(self.proba))]
        for key, values in self.proba.items():
            for value in values:
                if value not in prob[key]:
                    prob[key][value] = 1 / total_label
                else:
                    prob[key][value] += 1/total_label

            prob[key][-1] = 0.5 / total_label

        self.prob = prob


if __name__ == "__main__":
    from support_function.support import load_data
    path1 = "../MNIST/mnist_train.csv"
    f_train, l_train = load_data(path1, 'train')
    f_train = list(f_train)
    l_train = list(l_train)
    # print(l_train)

    naive = NaiveB([1, -1])
    naive.train(f_train, l_train)

    path2 = "../MNIST/mnist_test.csv"
    f_test, l_test = load_data(path2, 'test')
    f_test = list(f_test)
    l_test = list(l_test)

    precision = naive.predict(f_test, l_test)
    print(precision)
    # train = [[1, 1], [1, 2], [1, 2], [1, 1], [1, 1], [2, 1], [2, 2], [2, 2], [2, 3], [2, 3], [3, 3], [3, 2], [3, 2],
    #          [3, 3], [3, 3]]
    # label = [-1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1]
    # assert (len(train) == len(label))
    # naive = NaiveB([1, -1])
    # naive.train(train, label)
    #
    # test = [[2, 1]]
    # l_test = [-1]
    #
    # print(naive.predict(test, l_test))
