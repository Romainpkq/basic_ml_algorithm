# 朴素贝叶斯对条件概率做出了条件独立的假设
# input: n dim vector
# output: classes(c1, c2, c3, .. ,ck)
# train data is generated by the P(X, Y) 独立同分布

# need attention: 1. prior probability； P(Y=ck), 2. 条件概率：P(X=x|Y=ck)


class NaiveB:
    """
    A realisation of naive bayes
    """
    def __init__(self, classes):
        """
        :param classes: the values of classes, example: [1,2,3,4,5]
        :param feature_values: a dictionary, key is the index of the dimension, value：the possible value of the dimension
            shape: (number_feature, possible_value_feature)
        """
        self.classes = classes
        self.pro_priors = [0 for i in range(len(classes))]
        self.pro_cons = []

    def train(self, train_data, train_label):
        """
        :param train_data: the datasets of train, an list, (number_data, features)
        :param train_label: the label of according train dataset, an list, (number_data)
        Update the pro_prior and pro_con parameter
        """
        number_pro = [Probability(name) for name in self.classes]
        number_label = [0 for i in range(len(self.classes))]

        for i in range(len(train_label)):
            label_index = self.classes.index(train_label[i])
            number_label[label_index] += 1
            for j in range(len(train_data[i])):
                if j not in number_pro[label_index].proba:
                    number_pro[label_index].proba[j] = [train_data[i][j]]
                else:
                    number_pro[label_index].proba[j].append(train_data[i][j])

        # get the prior probability
        for i in range(len(self.classes)):
            self.pro_priors[i] = number_label[i]/len(train_label)

        for pro in number_pro:
            pro.get_proba()

        self.pro_cons = number_pro

    def predict(self, test_data, test_label):
        # test_data: the dataset of the test data
        # test_label: the label of the test data
        predict_label = []
        for j in range(len(test_data)):
            probability_classes = []
            for i in range(len(self.classes)):
                p = 1
                for m in range(len(self.pro_cons[i].prob)):
                    p *= self.pro_cons[i].prob[m].get(test_data[j][m], 0)  # 第i个类
                probability_classes.append(p)

            n = probability_classes.index(max(probability_classes))
            predict_label.append(n)

        correct = 0
        for i in range(len(test_label)):
            if predict_label[i] == test_label[i]:
                correct += 1
        return correct / len(test_label)


class Probability:
    def __init__(self, class_name):
        self.class_name = class_name
        self.proba = {}
        self.prob = []

    def get_proba(self):
        # prob: (num_features)
        total_label = len(self.proba[0])
        prob = [{} for i in range(len(self.proba))]
        for key, values in self.proba.items():
            for value in values:
                if value not in prob[key]:
                    prob[key][value] = 1/total_label
                else:
                    prob[key][value] += 1/total_label

        self.prob = prob


if __name__ == "__main__":
    from support_function.support import load_data
    path1 = "../MNIST/mnist_train.csv"
    f_train, l_train = load_data(path1, 'train')
    f_train = list(f_train)
    l_train = list(l_train)
    # print(l_train)

    naive = NaiveB([1, -1])
    naive.train(f_train, l_train)

    path2 = "../MNIST/mnist_test.csv"
    f_test, l_test = load_data(path2, 'test')
    f_test = list(f_test)
    l_test = list(l_test)

    precision = naive.predict(f_test, l_test)
    print(precision)
